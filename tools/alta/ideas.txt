* Prepare API with input image and output oracle score + Understand ROS
* Train segformer with no reweighting and compare to convnets (PSPnet, BiSEnet, FCN, Unet)
* How to use results from previous heights???
* Freeze some layers to reduce overfitting. Some nets don't have this option yet (e.g. segformer)
* Use more datasets to improve generalization (either pretrain or mix with Alta). List of relevant datasets exists in folder AI_datasets and also in a
document in my drive. Here you already have 'iSaid' dataset.
* Label smoothing - either uniform, or according to semantic closeness
* Display softmax score. It seems that smaller scores are obtained in the vicinity of edge pixels,
hence it can be used to eliminate small objects in high altitude!!!
It mainly depends on the decoder interpolation interval (psp-d8 - 8, segformer - 4)
In addition, some wrongly classified areas (e.g. inside buildings), also get low scores
* Change class weights? It might be unnecessary now, since results on training set are almost perfect, even with PSPnet.
* Use 'inter_area' instead of 'bilinear' interpolation, or an explicit antialiasing filter? It doesn't seem to help...

* 07/03/2022
* After resolution reduction *1/8 (approx.), finally the small columns aren't detected at high altitudes.
* DDU - Deterministic Deep Uncertainty - encouraging each class to behave like a Gaussian in the feature space, using
spectral normalization, Leaky-RELU and average pooling.
* There exist code for classification. I asked for the code for Segmentation, didn't get a response yet.
* Possible improvements: Relative Maha-distance; Near and Far OOD separation;
* Students project?
* Chaim Baskin

* 29/02/2022
* Metaformer - according to the paper, it's slightly better than pvt (on ADE dataset), which is much worse than
segformer, with similar speed. Maybe it's due to the segformer's decoder?
* Using antialiasing filter during inference only, the small columns disappear only at 15+ filter size (at 100m),
 and results overall decrease markedly.
* Using antialiasing filter during training and inference, ...?
* Showing the softmax output, or valuse above a certain threshold.
It seems that some of the wrong predictions have also lowish score (<0.9). But certainly not all.
But anyway, the score near edges are lower.
We should think how to use them...

* For my presentation
* Show presentation about the project as a whole
* Show the AirSim simulation
* Explain about Alta, how it was collected and annotated
* Explain about the annotation problems (both technical and essential)
* Explain about the mmsegmentation package
* Show difference between cnn and transformer using the mis-annotated image
* Students project to implement and optimise on Jetson AGX
* Yarin Gal paper on DDU in semantic segmentation? Ask for the specific code they used
... Will it help to use more than one Gaussian per class?


** on ISL15 RTX3090 you should install torch using (although there's only cuda10.2 installed!)
 pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html


# For meeting with Haim Baskin
* My aim: Develop a real-time OOD detector (epistemic uncertainty), which does not require external datasets.
 - Unlike Monte-Carlo Dropout and Deep Ensembles, which are not real time
 - Unlike many other methods which use external datasets for training and calibration.
* Yarin Gal paper on DDU in semantic segmentation?
 - using Spectral Normalization, Leaky Relu and Average Pooling in the Residual connections.
* Out Of Distribution Detection and Generation using Soft Brownian Offset Sampling and AutoEncoders
* VOS - Virtual Outliers Sampling.
In my opinion the Gaussian assumption is flawed. Also the optimization certainly can't converge, because new outliers change the loss!
* My idea (won't work) - same as VOS, only a more sophisticated outlier synthesis
 1) low-dim histogram sampling in random directions. Maybe it enables starting the loss training earlier
 2) A different uncertainty loss function. I don't fully understand why this works...
 3) Incorporate in other layers, since now the Gaussian assumption isn't needed
 4) What prevents getting a large value of (-energy) even after employing the method???
* My best idea thus far - measure the loss between the 1d pdf (or cdf), which requires only a small number of samples, with a
Gaussian pdf (or cdf) with the same mean and std. The sample pdf can be implemented using several sigmoid functions.
In each iteration, one can choose one dimension (random or cyclical), or several, or all - per class.
We can also choose
* Yaniv Romano - know his work!


###### put in inference_alta?
if not isinstance(results['img'], torch.Tensor):
    img = mmcv.imread(results['img'])
else:
    img = results['img']
