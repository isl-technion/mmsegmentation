* Prepare API with input image and output oracle score + Understand ROS
* Train segformer with no reweighting and compare to convnets (PSPnet, BiSEnet, FCN, Unet)
* How to use results from previous heights???
* Freeze some layers to reduce overfitting. Some nets don't have this option yet (e.g. segformer)
* Use more datasets to improve generalization (either pretrain or mix with Alta). List of relevant datasets exists in folder AI_datasets and also in a
document in my drive. Here you already have 'iSaid' dataset.
* Label smoothing - either uniform, or according to semantic closeness
* Display softmax score. It seems that smaller scores are obtained in the vicinity of edge pixels,
hence it can be used to eliminate small objects in high altitude!!!
It mainly depends on the decoder interpolation interval (psp-d8 - 8, segformer - 4)
In addition, some wrongly classified areas (e.g. inside buildings), also get low scores
* Change class weights? It might be unnecessary now, since results on training set are almost perfect, even with PSPnet.
* Use 'inter_area' instead of 'bilinear' interpolation, or an explicit antialiasing filter? It doesn't seem to help...

* 07/03/2022
* After resolution reduction *1/8 (approx.), finally the small columns aren't detected at high altitudes.
* DDU - Deterministic Deep Uncertainty - encouraging each class to behave like a Gaussian in the feature space, using
spectral normalization, Leaky-RELU and average pooling.
* There exist code for classification. I asked for the code for Segmentation, didn't get a response yet.
* Possible improvements: Relative Maha-distance; Near and Far OOD separation;
* Students project?
* Chaim Baskin

* 29/02/2022
* Metaformer - according to the paper, it's slightly better than pvt (on ADE dataset), which is much worse than
segformer, with similar speed. Maybe it's due to the segformer's decoder?
* Using antialiasing filter during inference only, the small columns disappear only at 15+ filter size (at 100m),
 and results overall decrease markedly.
* Using antialiasing filter during training and inference, ...?
* Showing the softmax output, or valuse above a certain threshold.
It seems that some of the wrong predictions have also lowish score (<0.9). But certainly not all.
But anyway, the score near edges are lower.
We should think how to use them...

* For my presentation
* Show presentation about the project as a whole
* Show the AirSim simulation
* Explain about Alta, how it was collected and annotated
* Explain about the annotation problems (both technical and essential)
* Explain about the mmsegmentation package
* Show difference between cnn and transformer using the mis-annotated image
* Students project to implement and optimise on Jetson AGX
* Yarin Gal paper on DDU in semantic segmentation? Ask for the specific code they used
... Will it help to use more than one Gaussian per class?


** on ISL15 RTX3090 you should install torch using (although there's only cuda10.2 installed!)
 pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html


# For meeting with Haim Baskin
* My aim: Develop a real-time OOD detector (epistemic uncertainty), which does not require external datasets.
 - Unlike Monte-Carlo Dropout and Deep Ensembles, which are not real time
 - Unlike many other methods which use external datasets for training and calibration.
* Yarin Gal paper on DDU in semantic segmentation?
 - using Spectral Normalization, Leaky Relu and Average Pooling in the Residual connections.
* Out Of Distribution Detection and Generation using Soft Brownian Offset Sampling and AutoEncoders
* Yaniv Romano - know his work!
* VOS - Virtual Outliers Sampling.
In my opinion the Gaussian assumption is flawed. Also the optimization certainly can't converge, because new outliers change the loss!
* My idea (won't work) - same as VOS, only a more sophisticated outlier synthesis
 1) low-dim histogram sampling in random directions. Maybe it enables starting the loss training earlier
 2) A different uncertainty loss function. I don't fully understand why this works...
 3) Incorporate in other layers, since now the Gaussian assumption isn't needed
 4) What prevents getting a large value of (-energy) even after employing the method???
* My best idea thus far - measure the loss between the 1d pdf (or cdf), which requires only a small number of samples, with a
Gaussian pdf (or cdf) with the same mean and std. The sample pdf can be implemented using several sigmoid functions.
In each iteration, one can choose one dimension (random or cyclical), or several, or all - per class.
We can also choose


# TODO:
* The whole method shuold be changed:
1) During a training epoch, do nothing, because the net's weights keep changing.
2) During a validation epoch, continuously gather mean, cov (as in now), and also estimate the histogram at many bins as possible
3) Once the epoch ends, make a few GD on the hist-loss alone.

* Check method on a single image? in that case, no approximations must be done
* can the hist loss be used alone - instead of CE??? Only if the statistics are continuously updated during training (IIR)!
* implement for batch size > 1, to increase sample size
* For Gaussian dist., maybe eliminate ~5-10% of the outliers, both for std estimation and for fitting the model.
* Related to the previous point, decrease importance of farthest bins
* Laplacian? Generalized Gaussian? ...
* Try on a different Dataset. Maybe PascalVoc2012, as in the paper DDU for semantic seg...
* Did they use (in DDU) the a-priori probabilities for each class in the final decision? And what about log(det(cov))?
Look at the DDU github...
* Can the method be used to improve transfer learning? Representation learning?
* allow var_sample to train? or at least try various values?
* After finishing training, check the fit to the model. Maybe use the model only in certain dimensions,
where the data fits the model well?
* When drawing from the exact model's dist., what is the resulting histogram?
* measure Gaussianity after training
* Distance measures - KL? L1smooth? MSE?
* disallow calculation for class 0 \ remove the zero label
* penalize diff(PxPy, Pxy), for any two random directions x,y. It shouldn't matter when using the full covariance matrix(?)...
* Refactor - use mboaz17 folders, reverse changes from their code ...
* Add the revision number to the folder name

# Done:
* fix resize_size, crop_size, check them in testing \ inference, then rerun segformer 672*448 training (even without histloss)
* Decimation of the label map instead of interpolation, for speed and memory
* no need to calculate target values for all classes - it's the same vector (after normalizing to 1)
* covariance instead of variance + ignore log(det(cov))?
* Make sure the hook epoch and the model epoch are the same
* to find OOD - save mean and covariance (or variances)
* implement random directions (should be easy)
* more bins
* examine the hist-loss without actually using it (by multiplying with 0) - it certainly decreases the hist loss (-:
* Don't update AgamimPathA100 (as of 24/3/2022) - it's wrong (DJI_146) !!!!!!!!!!!
* It seems that ~0.1/5000 is the minimum obtainable hist-loss (per class, for L1smooth, Alta, Segformer, penultimate layer).
Deducted from segformer_mit-b0_pathA_30_50_pathA_30_50_672_448_HL5000\20000





pip list (29/03/2022) - for Autonomous-landing API, revision 4be91cd7482d1386f51949d1bdc9d9c6128fb715
addict             2.4.0
attrs              21.4.0
certifi            2021.10.8
cycler             0.11.0
fonttools          4.28.5
importlib-metadata 4.10.0
iniconfig          1.1.1
kiwisolver         1.3.2
matplotlib         3.5.1
mkl-fft            1.3.1
mkl-random         1.2.2
mkl-service        2.4.0
mmcv-full          1.4.2
mmsegmentation     0.20.2    /home/airsim/repos/open-mmlab/mmsegmentation
numpy              1.21.2
olefile            0.46
opencv-python      4.5.5.62
packaging          21.3
Pillow             8.4.0
pip                21.2.2
pluggy             1.0.0
prettytable        2.5.0
py                 1.11.0
pyparsing          3.0.6
pytest             7.0.1
python-dateutil    2.8.2
PyYAML             6.0
scipy              1.7.3
setuptools         58.0.4
six                1.16.0
tomli              2.0.1
torch              1.10.1
torchaudio         0.10.1
torchvision        0.11.2
typing-extensions  3.10.0.2
wcwidth            0.2.5
wheel              0.37.0
yapf               0.32.0
zipp               3.7.0
