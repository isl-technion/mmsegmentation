* Prepare API with input image and output oracle score + Understand ROS
* Train segformer with no reweighting and compare to convnets (PSPnet, BiSEnet, FCN, Unet)
* How to use results from previous heights???
* Freeze some layers to reduce overfitting. Some nets don't have this option yet (e.g. segformer)
* Use more datasets to improve generalization (either pretrain or mix with Alta). List of relevant datasets exists in folder AI_datasets and also in a
document in my drive. Here you already have 'iSaid' dataset.
* Label smoothing - either uniform, or according to semantic closeness
* Display softmax score. It seems that smaller scores are obtained in the vicinity of edge pixels,
hence it can be used to eliminate small objects in high altitude!!!
It mainly depends on the decoder interpolation interval (psp-d8 - 8, segformer - 4)
In addition, some wrongly classified areas (e.g. inside buildings), also get low scores
* Change class weights? It might be unnecessary now, since results on training set are almost perfect, even with PSPnet.
* Use 'inter_area' instead of 'bilinear' interpolation, or an explicit antialiasing filter? It doesn't seem to help...

* 07/03/2022
* After resolution reduction *1/8 (approx.), finally the small columns aren't detected at high altitudes.
* DDU - Deterministic Deep Uncertainty - encouraging each class to behave like a Gaussian in the feature space, using
spectral normalization, Leaky-RELU and average pooling.
* There exist code for classification. I asked for the code for Segmentation, didn't get a response yet.
* Possible improvements: Relative Maha-distance; Near and Far OOD separation;
* Students project?
* Chaim Baskin

* 29/02/2022
* Metaformer - according to the paper, it's slightly better than pvt (on ADE dataset), which is much worse than
segformer, with similar speed. Maybe it's due to the segformer's decoder?
* Using antialiasing filter during inference only, the small columns disappear only at 15+ filter size (at 100m),
 and results overall decrease markedly.
* Using antialiasing filter during training and inference, ...?
* Showing the softmax output, or valuse above a certain threshold.
It seems that some of the wrong predictions have also lowish score (<0.9). But certainly not all.
But anyway, the score near edges are lower.
We should think how to use them...

* For my ATR presentation
* Show presentation about the project as a whole
* Show the AirSim simulation
* Explain about Alta, how it was collected and annotated
* Explain about the annotation problems (both technical and essential)
* Explain about the mmsegmentation package
* Show difference between cnn and transformer using the mis-annotated image
* Students project to implement and optimise on Jetson AGX
* Yarin Gal paper on DDU in semantic segmentation? Ask for the specific code they used
... Will it help to use more than one Gaussian per class?


# For the first meeting with Haim Baskin
* My aim: Develop a real-time OOD detector (epistemic uncertainty), which does not require external datasets.
 - Unlike Monte-Carlo Dropout and Deep Ensembles, which are not real time
 - Unlike many other methods which use external datasets for training and calibration.
* Yarin Gal paper on DDU in semantic segmentation?
 - using Spectral Normalization, Leaky Relu and Average Pooling in the Residual connections.
* Out Of Distribution Detection and Generation using Soft Brownian Offset Sampling and AutoEncoders
* Yaniv Romano - know his work!
* VOS - Virtual Outliers Sampling.
In my opinion the Gaussian assumption is flawed. Also the optimization certainly can't converge, because new outliers change the loss!
* My idea (won't work) - same as VOS, only a more sophisticated outlier synthesis
 1) low-dim histogram sampling in random directions. Maybe it enables starting the loss training earlier
 2) A different uncertainty loss function. I don't fully understand why this works...
 3) Incorporate in other layers, since now the Gaussian assumption isn't needed
 4) What prevents getting a large value of (-energy) even after employing the method???
* My best idea thus far - measure the loss between the 1d pdf (or cdf), which requires only a small number of samples, with a
Gaussian pdf (or cdf) with the same mean and std. The sample pdf can be implemented using several sigmoid functions.
In each iteration, one can choose one dimension (random or cyclical), or several, or all - per class.

# For the second meeting with Haim Baskin 17/05/2022
* Show my work up till now - presentation, papers
* Only then I discovered some other works...
* There are works on making the latent space Gaussian, using Wasserstein, SW meausure, Distributional SW,, Generalized SW,
Chi-squared on radii and differences, etc.
* However:
1) Their loss is on a single mini-batches, not the entire dataset.
2) The datasets are usually MNIST, CIFAR, CelebA, and LSUN (downsampled to 64x64).
3) The task is usually Auto-Encoder. If OOD is mentioned, it refers to the entire population, not a specific class.
I didn't find papers doing both classification\det\seg and OOD (maybe 'FOOD\GLOD', but they use a Gaussian layer).
* I use an aggregate measure (Kurtosis) on the entire epoch. I don't know what's the correct weight per batch. I also don't
know how to balance weights of different classes.
* How to adapt SW to the entire epoch? It seems impossible without saving lots of data...
* However, adapting the chi^2 on radii and differences is possible using density estimation. It won't blow the memory because
it's only one\two per class.
* It can be used both in ood and in AutoEncoders. It doesn't have to be Gaussian, according to the paper (Jarek Duda).
* Also, in segmentation (and detection), one cannot randomly sample any mini-batch as in classification. Hence, using
their methods might not work on the whole distribution.
* Regarding the small eigenvalues, maybe a different net will solve this? (e.g. deeplabv3+)


1) Is it possible to make all layers approx. Gaussian without harming results?
2) Using a different net, which will not collapse eigs?
3) Use a standard Dataset (e.g. Pascal VOC)
4) Try Chi^2 distribution to minimize.


# TODO:
* DeepLabV3+:
1) Put the loss before the ReLU, then put ReLU (fix the bug in the last DepthwiseSeparableConvModule!!!)
2) First class should be 0 not 1
3) Does BatchNorm behavious differ from train to val? If so, put Layer-Norm?
Because the loss_hist_dec_4 is much higher in val stage with the same dataset!!!
4) Use Leaky-ReLU\GeLU
5) Put weight-norm in the decoder as well
6) Use Local Contrast Norm?

* Implement the method for batch_size>1
* Implement histogram of radii and differences
* Use deeplabv3+ and voc. Are there any tiny eigen values?
* Update all PathA, pathB annotations!
* Use Leaky-Relu to prevent feature collapse
* Why we have sometimes kurtosis>0 but almost perfect Gaussian? Esp. for the small PCA's...
* Does GAN assume\force Normal distribution. Some new works do...
* Weighing the layers by their total loss
* How to weigh the loss per iteration of the epoch?????????????????????
* Increase loss weight of the PCA directions.
* Use two consecutive vals epochs at the end? Probably unnecessary...
* When I discard BN, it seems that the sum of effective eigenvalues of all classes is 31*8=~256 (in loss_hist_dec_0). In dec_1 it's 63*8.
 Why? Are the subspaces orthogonal?  Is it a function of the resolution?
The first ~31 eigenvalues are very similar as well. Why? Is it important? usable?
In dec_4 there is no sudden drop like in previous layers, and all eigs are large enough. Why? Does it happen also without hist loss?
* check directions that are nearly aligned with the smallest eigenvals! Will it still work?
* Put the hist loss before the norm layer??? because during training, the batch statistics are used, while in evaluation\testing,
the running statistics are used. Use track_running_stats=False in training, or use batch statistics.
Maybe it's better regardless, because the image can contain one area in dist. and the other outside...
* Try Deeplabv3 with unet
* Don't add cov_eps, because it affects the eigenvals. Simply don't measure the distance in these directions.
* Why do all 5 losses jump, occasionally? Is it simply an unexplored area of space? Probably because of the small eigenvals...
* Increase the number of directions.
* Is class 9 problematic? Eliminate too small classes (e.g. samples_num < const*dimension over an epoch)
* More measures of Gaussianity:
Multivariate skewness can encourage elliptic symmetrical pdfs - ideal for distance measure?
Also, it seems that some directions are skewed
* Use MCR2 in addition to my loss
* Does the number of effective eigen values increase following training?

* Choose a different kernel (e.g. 1/(1+x**2)) with less memory footprint. Also, change loss to L1?
* Also, try limiting the kernel's support (e.g. below 1e-20 is 0)
* delete data to save disk space.
* Think about the lr schedule

* Increase the loss weight gradually, to allow convergence to good segmentation first
* Alternatively, use the gaussian log-probs as logits? Does it matter? Probably not...
* change bins num and range, compare histograms only in miu+-k*std
* Randomly rotate the features around the current mean of each class, in each iteration, in order to 'fill' the Gaussian with samples

* The whole method shuold be changed:
1) During a training epoch, do nothing, because the net's weights keep changing.
2) During a validation epoch, continuously gather mean, cov (as in now), and also estimate the histogram at many bins as possible
3) Once the epoch ends, make a few GD on the hist-loss alone.

* Check method on a single image? in that case, no approximations must be done
* can the hist loss be used alone - instead of CE??? Only if the statistics are continuously updated during training (IIR)!
* implement for batch size > 1, to increase sample size
* For Gaussian dist., maybe eliminate ~5-10% of the outliers, both for std estimation and for fitting the model.
* Related to the previous point, decrease importance of farthest bins
* Laplacian? Generalized Gaussian? ...
* Try on a different Dataset. Maybe PascalVoc2012, as in the paper DDU for semantic seg...
* Did they use (in DDU) the a-priori probabilities for each class in the final decision? And what about log(det(cov))?
Look at the DDU github...
* Can the method be used to improve transfer learning? Representation learning?
* allow var_sample to train? or at least try various values?
* After finishing training, check the fit to the model. Maybe use the model only in certain dimensions,
where the data fits the model well?
* When drawing from the exact model's dist., what is the resulting histogram?
* measure Gaussianity after training
* Distance measures - KL? L1smooth? MSE?
* disallow calculation for class 0 \ remove the zero label
* penalize diff(PxPy, Pxy), for any two random directions x,y. It shouldn't matter when using the full covariance matrix(?)...
* Refactor - use mboaz17 folders, reverse changes from their code ...
* Add the revision number to the folder name

# Done:
* calculate mom4 correctly!
* Remove calculation of the histogram - it's time consuming!
* Split the features to groups (e.g. 10), and hopefully each group can be made Gaussian. Then sum the distances to each.
It seems unnecessary after removing the alpha filter, though.
* The relative weight of the current histogram in the loss is increased gradually during a batch until reaching 1
* The filtering parameter (alpha_hist) should be determined according to the samples number in the batch
* fix resize_size, crop_size, check them in testing \ inference, then rerun segformer 672*448 training (even without histloss)
* use validation epoch to save hist_model!
* Decimation of the label map instead of interpolation, for speed and memory
* no need to calculate target values for all classes - it's the same vector (after normalizing to 1)
* covariance instead of variance + ignore log(det(cov))?
* Make sure the hook epoch and the model epoch are the same
* to find OOD - save mean and covariance (or variances)
* implement random directions (should be easy)
* more bins
* examine the hist-loss without actually using it (by multiplying with 0) - it certainly decreases the hist loss (-:
* Don't update AgamimPathA100 (as of 24/3/2022) - it's wrong (DJI_146) !!!!!!!!!!!
* It seems that ~0.1/5000 is the minimum obtainable hist-loss (per class, for L1smooth, Alta, Segformer, penultimate layer).
Deducted from segformer_mit-b0_pathA_30_50_pathA_30_50_672_448_HL5000\20000

## For paper
* Add the 'void' category description and count it.

## Benchmarking issues and dilemmas - for paper
* What resolution? The full one is possible in training only when using cropping. Another option is to downscale the images.
* Which categories? With equal weight, some categories will not be detected.
* Write script to count pixels per category.
* What should be the class weights? UAVid didn't reweight
* What is the train, val and test sets? UAVid had 30 sequences - 15, 5, 10 respectively.
And the test was kept hidden for benchmarking (as in UAVid, SemanticDrone and Cityscape)
IrYamim could be the test set(?). Or maybe PilotPath? What about the descend scenarios? Another test set?
* Which architectures to try?
* Use batch-size>1. On one hand, it limits the crop size. On the other, batchnorm layer should be fixed to use only 1.
* Which augmentations to use? Should I use albumentations? Report with and without augs?
* Use a pretrained net, or scratch, or both (As in UAVid)?
* Metric: UAVid used mIoU (as Pascal-VOC as well)
* Compare to several datasets (UAVid, ISPRS, ICG SemanticDrone, Citscapes)
* Train each net with a single height, or all heights? Same dilemma for testing - report separately and\or together?
*
* Set running_location as an envoroment variable?
* Run segformer-m3?
* Write an automatic test code that also measures results
* Take new annotations (except descents) and also update class histogram diagram (including descents)
* Write in table inference time and memory usage and #params
* Set hyper-parameters (esp. iterations number) using the PathC val set (use tensorboard), or use number of epochs (e.g. 20)
* Mask the building areas from testing (and training?) to get reasonable results. Update class weights accordingly!!!
* Report the results of 15 categories and also super-categories (e.g. veg+rough+soft, water+bicycle+person) to get reasonable results?
* Report results of each height separately or everything together? Separately seems better. How to calculate the final result???
Onw part? Two parts? Five parts?  Keep it simple - don't split anything!!!
* Perform score calibration (??)

* Automatic training script, including several trials per model
* Automatic testing script, including averaging trials
* Unexplained stability problems of the GPU
* Setting number of iterations using the val set, not for all models, and without the Descent datasets
* Expanded the paper

UAVid:
Dataset - 30 scenarios divided to train, val test as 15, 5, 10
Image size ~ 4Kx2K. Divided to 3x3 crops of 2048x1024 (bit training and testing)
8 categories, the smallest (Human) has 0.16% of the pixels
Metric - mIoU
Architectures - FCN-8s, Dilation net and U-Net. Also, Multi-Scale-Dilation Net to deal with different object sizes
Loss - Cross Entropy (with auxiliary loss for the MS Net)
Pretraining - scratch\pretraining on Cityscapes
Postprocessing - spatio-temporal CRF
GPU - Nvidia Titan X GPU with 12G, one image per batch
Optimization - All models are trained with the Adam optimizer for 27K iterations (20 epochs). The base learning
rate is set to 10−4 exponentially decaying to 10−7. Weight decay for all weights in convolutional kernels is set to 10−5
Augmentation - left-to-right flip randomly; series of color augmentation, including random hue operation, random contrast operation, random
brightness operation, random saturation operation.
Quantitative results - comparative table
Qualitative reults - focusing on a specific area where using their MS-net is advantageous
Class weights - uniform




**pip list (29/03/2022) - for Autonomous-landing API, revision 4be91cd7482d1386f51949d1bdc9d9c6128fb715
addict             2.4.0
attrs              21.4.0
certifi            2021.10.8
cycler             0.11.0
fonttools          4.28.5
importlib-metadata 4.10.0
iniconfig          1.1.1
kiwisolver         1.3.2
matplotlib         3.5.1
mkl-fft            1.3.1
mkl-random         1.2.2
mkl-service        2.4.0
mmcv-full          1.4.2
mmsegmentation     0.20.2    /home/airsim/repos/open-mmlab/mmsegmentation
numpy              1.21.2
olefile            0.46
opencv-python      4.5.5.62
packaging          21.3
Pillow             8.4.0
pip                21.2.2
pluggy             1.0.0
prettytable        2.5.0
py                 1.11.0
pyparsing          3.0.6
pytest             7.0.1
python-dateutil    2.8.2
PyYAML             6.0
scipy              1.7.3
setuptools         58.0.4
six                1.16.0
tomli              2.0.1
torch              1.10.1
torchaudio         0.10.1
torchvision        0.11.2
typing-extensions  3.10.0.2
wcwidth            0.2.5
wheel              0.37.0
yapf               0.32.0
zipp               3.7.0


** on ISL15 RTX3090 you should install torch using (although there's only cuda10.2 installed!)
 pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html


** register new ssh key:
ssh-keygen -t ed25519
cat ~/.ssh/id_ed25519.pub
In github, generate new ssh key and copy the content, e.g.:
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMWelQMTz3wH3UGAU29HAb3CIN5ouHSlaEO6QPrHXuue airsim@isl15
